


from rime.process_config import *
from rime.rime_functions import *
from rime.utils import *

import dask
import dask.dataframe as dd
# from dask.diagnostics import ProgressBar
from dask.diagnostics import Profiler, ResourceProfiler, CacheProfiler, ProgressBar
from dask.distributed import Client

# from dask.distributed import Client # uncomment this to open Dask client to see performance
import glob
import numpy as np
import pandas as pd
import pyam
import re
import time
import xarray as xr
import yaml


# Load a yaml file that will help select multiple climate indicator files (not necessarily needed)
with open("indicator_params.yml", "r") as f:
    params = yaml.full_load(f)


dask.config.set(scheduler="processes")
dask.config.set(num_workers=num_workers)
client = Client()  # uncomment this to open Dask client








os.getcwd()


df_scens_in = pyam.IamDataFrame('test_data/emissions_temp_AR6_small.xlsx')
dft = df_scens_in.filter(variable=temp_variable) # temp_variable as defined in the config file, = 'AR6 climate diagnostics|Surface Temperature (GSAT)|MAGICCv7.5.3|50.0th Percentile'

# Replace & fill missing SSP scenario allocation
dft = ssp_helper(dft, ssp_meta_col="Ssp_family", default_ssp="SSP2")





scenarios = ["SSP1-26", "SSP2-45", "SSP3-Baseline", "SSP5-Baseline"]
variable = (
    "AR6 climate diagnostics|Surface Temperature (GSAT)|MAGICCv7.5.3|50.0th Percentile"
)
ssps = df_scens_in.filter(scenario=scenarios, model="IMAGE*", variable=variable)
imps = df_scens_in.filter(
    IMP_marker=["CurPol", "ModAct", "SP", "GS", "Neg"], variable=variable
)  # ,'Ren-2.0','Neg-2.0'
ssps_imp = ssps.append(imps)
ssps_imp = ssp_helper(ssps_imp, ssp_meta_col="Ssp_family", default_ssp="SSP2")
ssps_imp








# Requires some configuration - need to download the data from https://zenodo.org/records/10212339, and set the file paths in the file process_config.py



# ind = "pr"
# var = "pr"
# short = "r10"  # params["indicators"][ind][var]["short_name"]
ssp = "ssp2"
ftype = "abs"


# published data

# # Prepare data
# impact_data_dir = 'test_data//'
# files = glob.glob(f"{impact_data_dir}/{var}*{short}_*{ftype}.nc4")
# mapdata = xr.open_mfdataset(
#     files, preprocess=remove_ssp_from_ds, combine="nested", concat_dim="gmt"
# )
# mapdata = xr.concat([mapdata[x] for x in mapdata.data_vars], dim='gmt').to_dataset(name=f'{var}_{short}')
# mapdata = tidy_mapdata(mapdata)
# mapdata = mapdata.sortby("gmt").drop_duplicates("gmt")


# impact_data_dir = '/mnt/c/users/byers/iiasa/ECE.prog - Documents/Research Theme - NEXUS/Hotspots_Explorer_2p0/rcre_testing/rime_testdata'
# files = glob.glob(f"{impact_data_dir}/*{var}*{short}_*{ftype}.nc4")
# files



# Prepare data (using split_files on sharepoint)
# impact_data_dir = '/mnt/c/users/byers/iiasa/ECE.prog - Documents/Research Theme - NEXUS/Hotspots_Explorer_2p0/rcre_testing/rime_testdata'
impact_data_dir = 'test_data'
# indicators = ['dri','iavar']
ind = 'dri_qtot'

files = glob.glob(f"{impact_data_dir}/*{ind}_*{ftype}.nc4")
mapdata = xr.open_mfdataset(
    files, preprocess=remove_ssp_from_ds, combine="nested", concat_dim="gwl"
)
mapdata
# mapdata = xr.concat([mapdata[x] for x in mapdata.data_vars], dim='gwl').to_dataset(name=f'{var}_{short}')
mapdata = tidy_mapdata(mapdata)
# mapdata = mapdata.sortby("gwl").drop_duplicates("gwl")


mapdata


print("Test multiple IAM scenarios, 1 indicator")

start = time.time()

# Run function
# Using dask for small numbers of scenarios ~<10 can be slower due to spawning of workers.
# use_dask=True currently broken 
map_out_MS = map_transform_gwl_wrapper(
    ssps_imp,
    mapdata,
    years,
    caution_checks=True,
    use_dask=False,
    include_orig_gwl=False,
    gwl_name="gwl",
    drawdown_max=0.15,
    temp_min=1.2,
    temp_max=3.0,
    interpolation=0.01,
)

comp = dict(zlib=True, complevel=5)
encoding = {var: comp for var in map_out_MS.data_vars}
filename = f"scenario_maps_multiscenario_{ftype}_test_notebook.nc"
map_out_MS.to_netcdf(filename, encoding=encoding)

print("FINISHED Test multiple scenarios, 1 indicator")
print(f"{time.time()-start}")


map_out_MS








# Doesn't work -0
# AttributeError: 'DataArray' object has no attribute 'rename_vars'
# lines 230-233 in map_transform_gmt

gwl_name = "gwl"
print("Test 1 scenario, multiple indicators")
start = time.time()
ssp = "ssp2"
mapdata = xr.Dataset()

# impact_data_dir = '/mnt/p/watxene/ISIMIP_postprocessed/cse/split_files_fixed'
impact_data_dir = 'test_data'
indicators = ['dri','iavar']
for ind in indicators:

    print(ind)
    files = glob.glob(f"{impact_data_dir}/ISI*{ind}*_{ftype}*.nc4")
        
    a = xr.open_mfdataset(
            files, preprocess=remove_ssp_from_ds, combine="nested", concat_dim="gwl")
    for x in a.data_vars:
        mapdata[x] = a[x]


mapdata = tidy_mapdata(mapdata)
mapdata


os.getcwd()





map_out_MI = map_transform_gwl_wrapper(
    ssps_imp.filter(IMP_marker="CurPol"),
    mapdata,
    years,
    use_dask=False,
    include_orig_gwl=False,
    gwl_name="gwl",
    drawdown_max=0.15,
    temp_min=1.2,
    temp_max=3.0,
    interpolation=0.01,
)

comp = dict(zlib=True, complevel=5)
encoding = {var: comp for var in map_out_MI.data_vars}
filename = f"scenario_maps_multiindicator_{ftype}_test_notebook.nc"
# map_out_MI.to_netcdf(filename, encoding=encoding)

print("FINISHED 1 scenario, multiple indicators")
print(f"{time.time()-start}")





map_out_MI





filename = "test_map_notebook.html"
plot_maps_dashboard(
    map_out_MI,
    filename=filename,
    year=2063,
    cmap="magma_r",
    shared_axes=True,
    clim=None,
)
#os.startfile(filename) # Windows
import webbrowser
webbrowser.open(f'file://{os.path.realpath(filename)}')








filesall = glob.glob(fname_input_climate)
files = filesall  # [:6]
f = files[-0]
# load input climate impacts data file
ds = xr.open_mfdataset(f)
ds = ds.sel(year=years)
ds


varis = list(ds.data_vars.keys())[:lvaris]
dsi = ds[varis]
print(f"# of variables = {len(varis)}")





mode = "GMT"
if mode == "GMT":
    dfp = df_scens_in.filter(variable=temp_variable)
elif mode == "bypass":
    dfp = prepare_cumulative(df_scens_in, years=years, use_dask=True)
    ts = dfp.timeseries().apply(co2togmt_simple)
    ts = pyam.IamDataFrame(ts)
    ts.rename(
        {
            "variable": {ts.variable[0]: "RIME|GSAT_tcre"},
            "unit": {ts.unit[0]: "Â°C"},
        },
        inplace=True,
    )
    # Export data to check error and relationships
    # ts.append(dfp).to_csv('c://users//byers//downloads//tcre_gmt_output.csv')
    dfp = ts
    dfp.meta = df_scens_in.meta.copy()
dfp = dfp.filter(year=years)


# For testing on C1 scenarios
# few_scenarios = True
# very_few_scenarios = False

# if few_scenarios:
#     dfp = dfp.filter(Category=["C1*"])
#     if very_few_scenarios:
#         dfp = dfp.filter(model="REMIND 2.1*", scenario="*")

dfp = ssps_imp


# pre-prepare the dataset into correct format
#  Assign SSP to meta and select SSP2 in case SSP not present in name
dfp = ssp_helper(dfp, ssp_meta_col="Ssp_family", default_ssp="SSP2", keep_meta=False)

dft = dfp.timeseries()
dft = dft.join(dfp.meta["Ssp_family"]).reset_index()
# dft = dft.apply(fix_duplicate_temps, years=years, axis=1)





start = time.time()
year_res = 10
parallel = True
if parallel:
    """
    For parallel processing, convert dft as a wide IAMC pd.Dataframe
    into a dask.DataFrame.
    """
    ddf = dd.from_pandas(dft, npartitions=1000)

    # dfx = dft.iloc[0].squeeze()  # FOR DEBUIGGING THE FUNCTION
    outd = ddf.apply(
        table_impacts_gmt,
        dsi=dsi,
        ssp_meta_col="Ssp_family",
        axis=1,
        meta=("result", None),
    )

    with ProgressBar():
        # try:
        df_new = outd.compute(num_workers=num_workers)
else:
    df_new = dft.apply(table_impacts_gmt, dsi=dsi, axis=1)

expandeddGMT = pd.concat([df_new[x] for x in df_new.index])
print(f" Done:  {time.time()-start}")

filename = f"RIME_output_{region}_{year_res}yr.csv"

# expandedd.to_csv(filename, encoding="utf-8", index=False)
print(f" Saved: {region} yrs={year_res}\n  {time.time()-start}")
print(f"{len(dsi.data_vars)} variables, {len(dfp.meta)} scenarios")





expandeddGMT = pyam.IamDataFrame(expandeddGMT)


expandeddGMT.variable


# model = "IMAG*"
scenario = [
    "CO_Bridge",
    "EN_INDCi2030_3000f",
    "EN_NPi2020_400f_lowBECCS",
    # 'NGFS2_Current Policies',
    "SSP1-26",
    "SSP2-45",
    # 'SSP3-Baseline',
    # 'SSP5-Baseline',
    "SusDev_SDP-PkBudg1000",
]
variable = "RIME|sdii|Hazard|Risk score|Population weighted"
# variable = 'RIME|wsi|Exposure|Population|%'
variable = "RIME|cdd|Exposure|Population|%"
expandeddGMT.filter(variable=variable, scenario=scenario, region="GBR").plot()


expandeddGMT.filter(variable=variable, scenario=scenario, region="PAK").plot()


expandeddGMT.variable





imps.scenario


color_map = {
    "SSP1-26": "AR6-SSP1-2.6",
    "SSP2-45": "AR6-SSP2-4.5",
    "SSP3-Baseline": "AR6-SSP3-7.0",
    "SSP5-Baseline": "AR6-SSP5-8.5",
    "NGFS2_Current Policies": "AR6-IP-CurPol",
    # 'EN_NPi2020_900f': 'AR6-IMP-Neg2.0',
    "EN_INDCi2030_3000f": "AR6-IP-ModAct",
    # 'SSP2_openres_lc_50': 'AR6-IMP-Ren2.0',
    # AR6-IMP-GS',
    "CO_Bridge": "AR6-IMP-GS",
    "EN_NPi2020_400f_lowBECCS": "AR6-IMP-Neg",
    "SusDev_SDP-PkBudg1000": "AR6-IMP-SP",
}

pyam.run_control().update({"color": {"scenario": color_map}})

ssps_imp.filter(
    variable="AR6 climate diagnostics|Surface Temperature (GSAT)|MAGICCv7.5.3|50.0th Percentile",
    scenario="SSP*",
).plot(legend=False, fill_between=True, color="scenario", title="")


ssps_imp.filter(
    variable="AR6 climate diagnostics|Surface Temperature (GSAT)|MAGICCv7.5.3|50.0th Percentile"
).plot(legend=False, fill_between=True, color="scenario", title="")
